{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from typing import Dict\n",
    "from typing import Generator\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from evidently import ColumnType\n",
    "from evidently.future.datasets import DataDefinition\n",
    "\n",
    "from evidently.future.datasets import DatasetColumn\n",
    "\n",
    "from evidently.future.datasets import Descriptor\n",
    "from evidently.future.metric_types import Metric\n",
    "from evidently.future.datasets import Dataset\n",
    "from evidently.future.metric_types import MetricResult\n",
    "from evidently.future.metric_types import MetricTestResult\n",
    "from evidently.future.metric_types import SingleValue\n",
    "from evidently.future.metric_types import SingleValueTest\n",
    "from evidently.future.metric_types import MetricCalculation\n",
    "from evidently.future.metric_types import MetricId\n",
    "from evidently.future.metric_types import SingleValueMetric\n",
    "from evidently.future.metric_types import TResult\n",
    "from evidently.future.preset_types import PresetResult"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "de4b011f992f1165",
   "metadata": {},
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class TextLengthScorer(Descriptor):\n",
    "    def __init__(self, column_name: str, alias: Optional[str] = None):\n",
    "        super().__init__(alias or f\"{column_name}: Text Length\")\n",
    "        self._column_name = column_name\n",
    "\n",
    "    def generate_data(self, dataset: \"Dataset\") -> Union[DatasetColumn, Dict[str, DatasetColumn]]:\n",
    "        lengths = dataset.column(self._column_name).data.apply(len)\n",
    "        return DatasetColumn(type=ColumnType.Numerical, data=lengths)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dc99c9bcf41dd669",
   "metadata": {},
   "source": [
    "class ToxicityScorer(Descriptor):\n",
    "    def __init__(self, column_name: str, alias: Optional[str] = None):\n",
    "        super().__init__(alias or f\"{column_name}: Toxicity\")\n",
    "        self._column_name = column_name\n",
    "\n",
    "    def generate_data(self, dataset: \"Dataset\") -> Union[DatasetColumn, Dict[str, DatasetColumn]]:\n",
    "        from evidently.descriptors import ToxicityLLMEval\n",
    "        from evidently.options.base import Options\n",
    "\n",
    "        feature = ToxicityLLMEval().feature(self._column_name)\n",
    "        data = feature.generate_features(dataset.as_dataframe(), None, Options())\n",
    "        return {\n",
    "            col: DatasetColumn(type=feature.get_type(f\"{feature.get_fingerprint()}.{col}\"), data=data[col])\n",
    "            for col in data.columns\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "48fc3742208e6385",
   "metadata": {},
   "source": [
    "def my_scorer(data: DatasetColumn) -> DatasetColumn:\n",
    "    return DatasetColumn(type=data.type, data=data.data)\n",
    "\n",
    "def my_scorer2(dataset: Dataset) -> Union[DatasetColumn, Dict[str, DatasetColumn]]:\n",
    "    return {\"c1\": dataset.column(\"column_1\"), \"c2\": dataset.column(\"column_2\")}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "279361d330bca4d8",
   "metadata": {},
   "source": [
    "from evidently.future.datasets import ColumnInfo\n",
    "from evidently.future.descriptors import CustomColumnDescriptor\n",
    "from evidently.future.descriptors import CustomDescriptor\n",
    "from evidently.future.descriptors import TextLength\n",
    "\n",
    "data = pd.DataFrame(data={\"column_1\": [1, 2, 3, 4, -1, 5], \"column_2\": [\"a\", \"aa\", \"aaaa\", \"aaaaaaa\", \"a\", \"aa\"]})\n",
    "\n",
    "dataset = Dataset.from_pandas(\n",
    "    data,\n",
    "    data_definition=DataDefinition(\n",
    "        numerical_columns=[\"column_1\"],\n",
    "        categorical_columns=[\"column_2\"],\n",
    "    ),\n",
    "    descriptors=[\n",
    "        TextLength(\"column_2\", alias=\"column 2 length\"),\n",
    "        TextLength(\"column_2\", alias=\"column 2 length\"),\n",
    "        # ToxicityScorer(\"column_2\"),\n",
    "        CustomColumnDescriptor(\"column_2\", my_scorer, alias=\"column 2 custom function\"),\n",
    "        CustomDescriptor(my_scorer2, alias=\"global custom function\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "dataset.as_dataframe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9f49a28098d1cad0",
   "metadata": {},
   "source": [
    "from evidently.future.tests import Reference, eq\n",
    "from evidently.future.metric_types import BoundTest\n",
    "from evidently.future.report import Context\n",
    "from typing import Optional\n",
    "from typing import List\n",
    "from plotly.express import line\n",
    "\n",
    "\n",
    "class MyMaxMetric(SingleValueMetric):\n",
    "    column: str\n",
    "\n",
    "    def _default_tests(self) -> List[BoundTest]:\n",
    "        return [eq(0).bind_single(self.get_fingerprint())]\n",
    "\n",
    "    def _default_tests_with_reference(self) -> List[BoundTest]:\n",
    "        return [eq(Reference(relative=0.1)).bind_single(self.get_fingerprint())]\n",
    "\n",
    "# implementation\n",
    "class MaxMetricImplementation(MetricCalculation[SingleValue, MyMaxMetric]):\n",
    "    def calculate(self, context: Context, current_data: Dataset, reference_data: Optional[Dataset]) -> SingleValue:\n",
    "        x = current_data.column(self.metric.column).data\n",
    "        value = x.max()\n",
    "        result = SingleValue(value=value)\n",
    "        figure = line(x)\n",
    "        figure.add_hrect(6, 10)\n",
    "        #result.set_widget([plotly_figure(title=self.display_name(), figure=figure)])\n",
    "        return result\n",
    "\n",
    "    def display_name(self) -> str:\n",
    "        return f\"Max value for {self.metric.column}\"\n",
    "\n",
    "\n",
    "from evidently.future.report import Context\n",
    "context = Context(None)\n",
    "\n",
    "context.init_dataset(dataset, None)\n",
    "\n",
    "result = MyMaxMetric(column=\"column_1\").to_calculation().call(context)\n",
    "result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9566651ec9d1bee1",
   "metadata": {},
   "source": [
    "from evidently.future.metrics.group_by import GroupBy\n",
    "from evidently.future.metric_types import render_results\n",
    "\n",
    "context = Context(None)\n",
    "\n",
    "context.init_dataset(dataset, None)\n",
    "\n",
    "metrics = GroupBy(MyMaxMetric(column=\"column 2 length\"), \"column_1\").generate_metrics(context)\n",
    "\n",
    "results = [metric.call(context) for metric in metrics]\n",
    "\n",
    "render_results(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8e8eaadc002b5d1",
   "metadata": {},
   "source": [
    "results[0][0].value"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83ca61bb9521db5f",
   "metadata": {},
   "source": [
    "from evidently.future.preset_types import MetricPreset\n",
    "from evidently.future.metrics import MinValue\n",
    "from evidently.future.metrics import MaxValue\n",
    "\n",
    "class ColumnSummary(MetricPreset):\n",
    "    def __init__(self, column: str):\n",
    "        self._column = column\n",
    "\n",
    "    def metrics(self) -> List[Metric]:\n",
    "        return [\n",
    "            MinValue(column=self._column),\n",
    "            MaxValue(column=self._column),\n",
    "        ]\n",
    "    \n",
    "    def calculate(self, metric_results: Dict[MetricId, MetricResult]) -> PresetResult:\n",
    "        return PresetResult(widget=[\n",
    "            *metric_results[MinValue(column=self._column).get_metric_id()].widget,\n",
    "            *metric_results[MaxValue(column=self._column).get_metric_id()].widget,\n",
    "        ])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2334f18d685745c3",
   "metadata": {},
   "source": [
    "from evidently.future.report import Report\n",
    "from evidently.future.tests import lte\n",
    "\n",
    "report = Report([\n",
    "    MyMaxMetric(column=\"column 2 length\", tests=[lte(100), lte(3)]),\n",
    "    MyMaxMetric(column=\"column_1\", tests=[lte(100)]),\n",
    "    MyMaxMetric(column=\"global custom function.c1\", tests=[lte(100)]),\n",
    "    ColumnSummary(\"column_1\"),\n",
    "    GroupBy(MyMaxMetric(column=\"column 2 length\"), \"column_1\"),\n",
    "])\n",
    "snapshot = report.run(dataset, None)\n",
    "snapshot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75eb724a58024000",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
